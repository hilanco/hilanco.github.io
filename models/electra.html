<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>HILANCO</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.1/css/bulma.min.css" integrity="sha512-ZRv40llEogRmoWgZwnsqke3HNzJ0kiI0+pcMgiz2bxO6Ew1DVBtWjVn0qjrXdT3+u+pSN36gLgmJiiQ3cQtyzA==" crossorigin="anonymous" />
        <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
    </head>
    <body>
        <section class="section">
            <div class="container">
			  <a href="https://hilanco.github.io/">
              <div class="columns">
                <div class="column is-8">				
                  <h1 class="title">
                    HILANCO
                  </h1>
                  <p class="subtitle">
                    Hungarian Intelligent Language Applications Consortium
                  </p>    
                </div>
                <div class="column is-4">
                  <figure class="image is-2-by-1">
                    <img src="../pictures/hilanco.png">
                  </figure>
                </div>
                </div>
				</a>
			</div>
		</section>
		<section class="section">
			<div class="container">
			<a class="button" href="https://hilanco.github.io">&laquo; Back to Hompage</a><br /><br />
                <div class="content">
					<h4 class="title is-4">
						HIL-ELECTRA
					</h4>
					<p class="has-text-justified">
						ELECTRA is based on the GAN (Generative adversarial network) method. Two ELECTRA models were trained:
						<ul>
							<li><strong>ELECTRA wiki:</strong> Trained on Hungarian Wikipedia. Training time: ~5 days.</li>					
							<li><strong>ELECTRA NYTI:</strong> Trained on NYTI v1 corpus (contains the Hungarian Wikipedia). Training time: ~7 days.</li>
						</ul>
						Both models were trained on 1 single GeForce RTX 2080 Ti type video card. Both pretraining was done in 1 million steps, with a batch size of 80. The vocabulary size was 64.000. 
						<br />For further details see: <a href="https://hilanco.github.io/papers/electra.pdf" target="_blank">HIL-ELECTRA paper</a>
					</p>
				</div>
            </div>
          </section>		  
            <section class="section">
				<div class="container">
                    <h4 class="title is-4">
                        References
                    </h4>
                    <div class="content">
                        <ul>
                            <li><a href="https://openreview.net/pdf?id=r1xMH1BtvB" target="_blank">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></li>
							<li><a href="https://github.com/google-research/electra" target="_blank">Github: ELECTRA</a></li>
                        </ul>
                    </div>
                </div> 
            </section>		  
          <section class="section">
            <div class="container">
                <h4 class="title is-4">
                    More Language Models
                </h4>
                
				<div class="columns is-multiline">
                    
                        <div class="column is-one-quarter">
                            <a href="https://hilanco.github.io/models/hilbert.html">
                            <div class="box">
                                <article class="media">
                                  <div class="media-content">
                                    <div class="content">
                                      <p>
                                        <strong>HILBERT</strong>
                                        <br />
                                        HILBERT trained on NYTI corpus 
                                      </p>
                                    </div>
                                  </div>
                                </article>
                            </div>
                            </a>    
                        </div> 
                    <div class="column is-one-quarter">
                    <a href="https://hilanco.github.io/models/roberta.html">
                        <div class="box">
                            <article class="media">
                              <div class="media-content">
                                <div class="content">
                                  <p>
                                    <strong>HIL-RoBERTa</strong>
                                    <br />
									&raquo; HIL-RoBERTa wiki<br />- trained on Hungarian Wikipedia	
                                  </p>
                                </div>
                              </div>
                            </article>
                        </div>
                    </a>
                    </div>
                    <div class="column is-one-quarter">
                        <a href="https://hilanco.github.io/models/albert.html">
                        <div class="box">
                            <article class="media">
                              <div class="media-content">
                                <div class="content">
                                  <p>
                                    <strong>HIL-ALBERT</strong>
                                    <br />
									&raquo; HIL-ALBERT NYTI <br />- trained on 10% of NYTI corpus									
									<br />
								    &raquo; HIL-ALBERT wiki <br />- trained on Hungarian Wikipedia
                                  </p>
                                </div>
                              </div>
                            </article>
                        </div>
                        </a>
                    </div>
                </div>
			</section>
    </body>
</html>