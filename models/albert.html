<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>HILANCO</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.1/css/bulma.min.css" integrity="sha512-ZRv40llEogRmoWgZwnsqke3HNzJ0kiI0+pcMgiz2bxO6Ew1DVBtWjVn0qjrXdT3+u+pSN36gLgmJiiQ3cQtyzA==" crossorigin="anonymous" />
        <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
		<style>
			.section { padding-top: 20px !important; padding-bottom: 0px}
		</style>
    </head>
    <body>
        <section class="section">
            <div class="container">
			<a href="https://hilanco.github.io/">
              <div class="columns">
                <div class="column is-8">				
                  <h1 class="title">
                    HILANCO
                  </h1>
                  <p class="subtitle">
                    Hungarian Intelligent Language Applications Consortium
                  </p>    
                </div>
                <div class="column is-4">
                  <figure class="image is-2-by-1">
                    <img src="../pictures/hilanco.png">
                  </figure>
                </div>
                </div>
				</a>
			</div>
		</section>
		<section class="section">
			<div class="container">
			  <a class="button" href="https://hilanco.github.io">&laquo; Back to Home Page</a><br /><br />
                <div class="content">
					<h4 class="title is-4">
						HIL-ALBERT
					</h4>
                  <p class="has-text-justified">
                    ALBERT is a language model aimed at improving training speed and decreasing memory consumption of the BERT model by applying parameter-reduction techniques. It was introduced in the original paper by Lan et al. We present two pre-trained uncased ALBERT models:  one of them was trained on Hungarian Wikipedia, which is a part of the Webcorpus 2.0 dataset, while the other one was trained on a sample from the NYTI-BERT corpus containing approximately 10% of the whole dataset. We used Googleâ€™s SentencePiece for tokenization with a vocabulary size of 30000 tokens. The models were trained using Masked Language Modeling but without Next Sentence Prediction. Our code was based on the Hugging Face library. The training was performed on 4 GTX 1080Ti GPU cards with the batch size set to 32. We used a single epoch for training the first model on the sample from the NYTI-BERT corpus and 2 epochs for training the other model on the Wikipedia corpus. In the first case, the run took approximately 85 hours and about 54 hours in the second case.
					<br />
					<br />
					<strong>To DOWNLOAD the models, please fill out the registration form: <a href="https://forms.gle/Cmr7QKToyBBFNKAi8" target="_blank">&raquo; REGISTRATION FORM &laquo;</a></strong>
                  </p>
				</div>

            </div>
          </section>		  
            <section class="section">
				<div class="container">
                    <h4 class="title is-4">
                        References
                    </h4>
                    <div class="content">
                        <ul>
                            <li><a href="https://arxiv.org/abs/1909.11942" target="_blank">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></li>
							<li><a href="https://github.com/google-research/albert" target="_blank">Github: ALBERT</a></li>
                        </ul>
                    </div>
                </div> 
            </section>
		  <hr />		  
          <section class="section">
            <div class="container">
                <h4 class="title is-4">
                    More Language Models
                </h4>
                
				<div class="columns is-multiline">
                    
                        <div class="column is-one-quarter">
                            <a href="https://hilanco.github.io/models/hilbert.html">
                            <div class="box">
                                <article class="media">
                                  <div class="media-content">
                                    <div class="content">
                                      <p>
                                        <strong>HILBERT</strong>
                                        <br />
                                        HILBERT is the BERT-Large model for Hungarian Trained on the 4 BN NYTI-BERT corpus. One of the pioneers of the revolutionary transformer models, BERT has caused a sweeping success in the field of neural NLP.
                                      </p>
                                    </div>
                                  </div>
                                </article>
                            </div>
                            </a>    
                        </div> 
                    
                  
                    <div class="column is-one-quarter">
                        <a href="https://hilanco.github.io/models/electra.html">
                        <div class="box">
                            <article class="media">
                              <div class="media-content">
                                <div class="content">
                                  <p>
                                    <strong>HIL-ELECTRA</strong>
                                    <br />
									&raquo; HIL-ELECTRA NYTI <br />- trained on NYTI-BERT corpus
									
									<br />
								    &raquo; HIL-ELECTRA wiki <br />- trained on Hungarian Wikipedia
                                  </p>
                                </div>
                              </div>
                            </article>
                        </div>
                        </a>
                    </div>
                    <div class="column is-one-quarter">
                    <a href="https://hilanco.github.io/models/roberta.html">
                        <div class="box">
                            <article class="media">
                              <div class="media-content">
                                <div class="content">
                                  <p>
                                    <strong>HIL-RoBERTa</strong>
                                    <br />
									&raquo; HIL-RoBERTa wiki<br />- trained on Hungarian Wikipedia	
                                  </p>
                                </div>
                              </div>
                            </article>
                        </div>
                    </a>
                    </div>
                </div>
			</section>
    </body>
</html>