<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>HILANCO</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.1/css/bulma.min.css" integrity="sha512-ZRv40llEogRmoWgZwnsqke3HNzJ0kiI0+pcMgiz2bxO6Ew1DVBtWjVn0qjrXdT3+u+pSN36gLgmJiiQ3cQtyzA==" crossorigin="anonymous" />
        <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
		 <style>
		  .b-border {font-weight: bold;}
		 </style> 
    </head>
    <body>
        <section class="section">
            <div class="container">
			  <a href="https://hilanco.github.io/">
              <div class="columns">
                <div class="column is-8">				
                  <h1 class="title">
                    HILANCO
                  </h1>
                  <p class="subtitle">
                    Hungarian Intelligent Language Applications Consortium
                  </p>    
                </div>
                <div class="column is-4">
                  <figure class="image is-2-by-1">
                    <img src="../pictures/hilanco.png">
                  </figure>
                </div>
                </div>
				</a>
			</div>
		</section>
		<section class="section">
			<div class="container">
			<a class="button" href="https://hilanco.github.io">&laquo; Back to Hompage</a><br /><br />
                <div class="content">
					<h4 class="title is-4">
						HIL-RoBERTa
					</h4>
                  <p class="has-text-justified">
                    Case based RoBERTa model trained on Hungarian Wikipedia published as part of the Webcorpus 2.0 dataset.<br />
					Pretraining was done in 1.25 million steps, with a batch size of 32, using a BPE encoded vocabulary of 30000 subwords.<br />
					Using a learning rate of 1e-4, the training went on for five epochs. On a configuration consisting of 4 GTX 1080Ti GPU cards with a total of 44 GB vRAM, the training took 219 hours.<br />
					For further details see: <a href="https://hilanco.github.io/papers/roberta.pdf" target="_blank">this page &raquo;</a>
                  </p>
				</div>

            </div>
          </section>		  
            <section class="section">
				<div class="container">
                    <h4 class="title is-4">
                        References
                    </h4>
                    <div class="content">
                        <ul>
                            <li><a href="https://arxiv.org/abs/1907.11692" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li>
							<li><a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta" target="_blank">Github: RoBERTa</a></li>
                        </ul>
                    </div>
                </div> 
            </section>	  
          <section class="section">
            <div class="container">
                <h4 class="title is-4">
                    More Language Models
                </h4>
                
				<div class="columns is-multiline">
                    
                        <div class="column is-one-quarter">
                            <a href="https://hilanco.github.io/models/hilbert.html">
                            <div class="box">
                                <article class="media">
                                  <div class="media-content">
                                    <div class="content">
                                      <p>
                                        <strong>HILBERT</strong>
                                        <br>
                                        HILBERT trained on NYTI corpus
                                      </p>
                                    </div>
                                  </div>
                                </article>
                            </div>
                            </a>    
                        </div> 
                    
                  
                    <div class="column is-one-quarter">
                        <a href="https://hilanco.github.io/models/electra.html">
                        <div class="box">
                            <article class="media">
                              <div class="media-content">
                                <div class="content">
                                  <p>
                                    <strong>HIL-ELECTRA</strong>
                                    <br />
									&raquo; HIL-ELECTRA NYTI <br />- trained on NYTI corpus
									
									<br />
								    &raquo; HIL-ELECTRA wiki <br />- trained on Hungarian Wikipedia
                                  </p>
                                </div>
                              </div>
                            </article>
                        </div>
                        </a>
                    </div>
                    <div class="column is-one-quarter">
                        <a href="https://hilanco.github.io/models/albert.html">
                        <div class="box">
                            <article class="media">
                              <div class="media-content">
                                <div class="content">
                                  <p>
                                    <strong>HIL-ALBERT</strong>
                                    <br />
									&raquo; HIL-ALBERT NYTI <br />- trained on 10% of NYTI corpus									
									<br />
								    &raquo; HIL-ALBERT wiki <br />- trained on Hungarian Wikipedia
                                  </p>
                                </div>
                              </div>
                            </article>
                        </div>
                        </a>
                    </div>
                </div>
			</section>
			<a class="button" href="https://hilanco.github.io">&laquo; Back to Hompage</a>
    </body>
</html>